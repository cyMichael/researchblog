# Representation Learning

To start this book, I would like to discuss representation learning. I got greatly inspired from this topic during my previous works - though they are mainly about dialogue systems. It's interesting to see how those high-level applications are tied with these basic concepts.

Representation learning is a broad concept. It entangles autoencoders, VAEs, GANs and many other techniques. But all of them seeks for one common objective -  find a good representation of underlying data.

Representation learning also enables some cool ways to train models. It explains how models benefit from pretrained models, and how we may better generalize our model. It also provides theoretical supports for semi-supervised learning and unsupervised learning.

Do remember that this is a blog but not a wiki for representation learning. Instead, it will only contains what I have learned but may be augmented along with my study. Some contents may be written in Chinese. The organization of this section will be as follows:

* Concept: Representation Learning
* Technique: Latent variables modeling in language generation models



